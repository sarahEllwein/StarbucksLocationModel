{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f903e144",
   "metadata": {},
   "source": [
    "# Starbucks Stores Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f371ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226cd2c",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f102a",
   "metadata": {},
   "source": [
    "Data Constraints:\n",
    "- Both Starbucks and US datasets published in 2017.\n",
    "- Starbucks store locations limited to US country. \n",
    "- Starbucks store limited to Starbucks brand (no Teavana)\n",
    "- Exclude Puerto Rico from US datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aafdeeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "starbucks = pd.read_csv('data/directory.csv')\n",
    "starbucks = starbucks.query(\"Brand == 'Starbucks'\").query(\"Country == 'US'\")\n",
    "starbucks = starbucks.drop(columns=[\"Brand\", \"Store Name\", \"Ownership Type\", \"Street Address\",\"Phone Number\",\"Timezone\", \"Postcode\", \"Country\"])\n",
    "starbucks = starbucks.rename(columns={'State/Province' : 'State'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4d3d7776",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv('data/uscities.csv')\n",
    "cities = cities[[\"city\", \"state_id\", \"state_name\", \"county_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9b8caf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic = pd.read_csv('data/demo.csv', encoding='cp1252')\n",
    "demographic = demographic[demographic['State'] != 'Puerto Rico']\n",
    "demographic[\"County\"] = demographic[\"County\"].apply(lambda x: ' '.join(x.split()[0:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04846a27",
   "metadata": {},
   "source": [
    "### Data Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2ac869ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.merge(starbucks, cities, left_on=[\"City\", \"State\"], right_on=[\"city\", \"state_id\"])\n",
    "mapping = mapping.drop(columns=[\"state_id\", \"city\", \"State\"])\n",
    "mapping = mapping.rename(columns={\"state_name\":\"State\", \"county_name\":\"County\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "695c1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "storecount = mapping.groupby(['County', 'State'])['Store Number'].count().to_frame().reset_index()\n",
    "storecount = storecount.rename(columns={\"Store Number\":\"Count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf696fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = storecount.merge(demographic, how='right', left_on=['County', 'State'], right_on=['County', 'State']).drop(columns=[\"Unnamed: 0\", \"CountyId\", \"VotingAgeCitizen\"])\n",
    "df['Count'] = df['Count'].fillna(0)\n",
    "df['Men'] = (df['Men']/df['TotalPop'])*100\n",
    "df['Women'] = (df['Women']/df['TotalPop'])*100\n",
    "df['Employed'] = (df['Employed']/df['TotalPop'])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82cba0",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c5956886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Count\n",
      "TotalPop      0.896795\n",
      "White        -0.202117\n",
      "Asian         0.450766\n",
      "Income        0.233709\n",
      "IncomePerCap  0.256303\n",
      "Professional  0.247451\n",
      "Construction -0.212778\n",
      "Transit       0.327334\n"
     ]
    }
   ],
   "source": [
    "var = ['Count', 'TotalPop', 'Men', 'Women', 'Hispanic',\n",
    "       'White', 'Black', 'Native', 'Asian', 'Pacific', 'Income', 'IncomeErr',\n",
    "       'IncomePerCap', 'IncomePerCapErr', 'Poverty', 'ChildPoverty',\n",
    "       'Professional', 'Service', 'Office', 'Construction', 'Production',\n",
    "       'Drive', 'Carpool', 'Transit', 'Walk', 'OtherTransp', 'WorkAtHome',\n",
    "       'MeanCommute', 'Employed', 'PrivateWork', 'PublicWork', 'SelfEmployed',\n",
    "       'FamilyWork', 'Unemployment']\n",
    "corr = df[var].corr().drop('Count')[['Count']]\n",
    "best_corr = corr[abs(corr[\"Count\"])>.20]\n",
    "print(best_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed5f45b",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "302a298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "feature_names = ['TotalPop', 'Hispanic', 'White', 'Black', 'Native', 'Asian', 'Pacific', 'IncomePerCap', 'Poverty',\\\n",
    "                 'Professional', 'Service', 'Office', 'Construction', 'Production', 'Drive', 'Carpool', 'Transit',\\\n",
    "                 'Walk', 'OtherTransp']\n",
    "features = df[feature_names].fillna(0)\n",
    "features = features.apply(lambda x: stats.zscore(x))\n",
    "target = df[[\"Count\"]].fillna(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "90c5904d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time                        0.005454\n",
      "score_time                      0.003966\n",
      "test_r2                         0.760518\n",
      "test_neg_mean_squared_error   -78.506952\n",
      "dtype: float64\n",
      "fit_time                         0.019534\n",
      "score_time                       0.003925\n",
      "test_r2                          0.422705\n",
      "test_neg_mean_squared_error   -114.892441\n",
      "dtype: float64\n",
      "fit_time                       6.657817e-01\n",
      "score_time                     8.738041e-03\n",
      "test_r2                       -3.173584e+03\n",
      "test_neg_mean_squared_error   -1.902187e+06\n",
      "dtype: float64\n",
      "fit_time                       1.150140e+00\n",
      "score_time                     2.017798e-02\n",
      "test_r2                       -8.768581e+03\n",
      "test_neg_mean_squared_error   -3.235406e+06\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "for i in range(1,5):\n",
    "    x_train_temp, x_test_temp, y_train_temp, y_test_temp = train_test_split(x_train, y_train, test_size=0.2, random_state=i)\n",
    "    poly_model = make_pipeline(PolynomialFeatures(i), LinearRegression())\n",
    "    d = pd.DataFrame(cross_validate(poly_model, x_train_temp, y_train_temp, cv=5, scoring=('r2', 'neg_mean_squared_error')))\n",
    "    print(d.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69156b53",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "924f00b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6306102723265778\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "map_ = np.vectorize(lambda x: 0 if x < 0 else math.floor(x))\n",
    "y_pred = np.array([map_(y) for y in model.predict(x_test)])\n",
    "\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86130a5",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fd75f147",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Non White'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-1fd649b5e91b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TotalPop\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"White\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Non White\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"IncomePerCap\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Professional\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Construction\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Transit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"TotalPop\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"White\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Non White\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"IncomePerCap\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Professional\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Construction\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Transit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Non White'] not in index\""
     ]
    }
   ],
   "source": [
    "def normalize(df, columns):\n",
    "    result = df[columns]\n",
    "    result = (result - result.mean())/result.std()\n",
    "    return result\n",
    "\n",
    "data = df[[\"TotalPop\",\"White\",\"Non White\",\"IncomePerCap\",\"Professional\",\"Construction\",\"Transit\"]]\n",
    "columns = [\"TotalPop\",\"White\",\"Non White\",\"IncomePerCap\",\"Professional\",\"Construction\",\"Transit\"]\n",
    "data = normalize(data, columns)\n",
    "data = data.join(df['Count'])\n",
    "\n",
    "# Feature = TotalPop\n",
    "data.plot.scatter(x=\"TotalPop\", y=\"Count\",s=1)\n",
    "plt.ylabel(\"# of Starbucks\")\n",
    "plt.xlabel(\"Total Population\")\n",
    "plt.title(\"Number of Starbucks by Feature\")\n",
    "\n",
    "# Feature = White Population\n",
    "data.plot.scatter(x=\"White\", y=\"Count\",s=1)\n",
    "plt.ylabel(\"# of Starbucks\")\n",
    "plt.xlabel(\"Proportiona of Population that is White\")\n",
    "plt.title(\"Number of Starbucks by Feature\")\n",
    "\n",
    "# Feature = Non White Population\n",
    "data.plot.scatter(x=\"Non White\", y=\"Count\",s=1)\n",
    "plt.ylabel(\"# of Starbucks\")\n",
    "plt.xlabel(\"Proportion of Population that is Non White\")\n",
    "plt.title(\"Number of Starbucks by Feature\")\n",
    "\n",
    "# Feature = Income Per Capita\n",
    "data.plot.scatter(x=\"IncomePerCap\", y=\"Count\",s=1)\n",
    "plt.ylabel(\"# of Starbucks\")\n",
    "plt.xlabel(\"Income Per Capita\")\n",
    "plt.title(\"Number of Starbucks by Feature\")\n",
    "\n",
    "# Feature = Professional Population\n",
    "data.plot.scatter(x=\"TotalPop\", y=\"Count\",s=1)\n",
    "plt.ylabel(\"# of Starbucks\")\n",
    "plt.xlabel(\"Proportion of Population that are Professionals\")\n",
    "plt.title(\"Number of Starbucks by Feature\")\n",
    "\n",
    "# Feature = Construction Population\n",
    "data.plot.scatter(x=\"Construction\", y=\"Count\",s=1)\n",
    "plt.ylabel(\"# of Starbucks\")\n",
    "plt.xlabel(\"Proportion of Population that work in Construction\")\n",
    "plt.title(\"Number of Starbucks by Feature\")\n",
    "\n",
    "# Feature = White Population\n",
    "data.plot.scatter(x=\"Transit\", y=\"Count\",s=1)\n",
    "plt.ylabel(\"# of Starbucks\")\n",
    "plt.xlabel(\"Proportion of Population that work in Transit\")\n",
    "plt.title(\"Number of Starbucks by Feature\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
